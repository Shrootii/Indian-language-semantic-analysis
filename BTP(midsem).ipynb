{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Dp-wfWUh16"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycpm1N6WOCsg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i56upFoqOC1y"
      },
      "outputs": [],
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_0a03gSOYhD"
      },
      "outputs": [],
      "source": [
        "##DATA PREPROCESSING AND TOKENISING\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the Kaggle dataset without header\n",
        "file_path = r'hindi sentiment analysis.csv'\n",
        "df = pd.read_csv(file_path, header=None, names=['text', 'label'], skiprows=1)  # Skip the first row\n",
        "\n",
        "# Assuming the first column contains Hindi text\n",
        "hindi_text = df['text'].astype(str).tolist()\n",
        "\n",
        "hindi_text = [re.sub(r'[^ ँ-ःअ-ऋए-ऑओ-नप-रल-ळव-हा़ी-ूॅ-ैॉ-ोौ्]', '', text) for text in hindi_text]\n",
        "hindi_text = [re.sub(r'\\s+', ' ', text).strip() for text in hindi_text]\n",
        "\n",
        "# Find the maximum length of text\n",
        "max_length = 0\n",
        "for text in hindi_text:\n",
        "    words = text.strip().split()\n",
        "    num_words = len(words)\n",
        "\n",
        "    if num_words > max_length:\n",
        "        max_length = num_words\n",
        "\n",
        "# Pad all other texts to the maximum length with a neutral Hindi word\n",
        "neutral_word = 'न्यूट्रलस'  # Replace this with an appropriate neutral word\n",
        "padded_sequences = []\n",
        "\n",
        "for text in hindi_text:\n",
        "    words = text.strip().split()\n",
        "    num_words = len(words)\n",
        "\n",
        "    padding_length = max_length - num_words\n",
        "    padded_text = text + (' ' + neutral_word) * padding_length\n",
        "    padded_sequences.append(padded_text)\n",
        "\n",
        "# Tokenize the Hindi text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(padded_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(padded_sequences)\n",
        "\n",
        "# Padding sequences for consistent length (if needed)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Create a new DataFrame with the padded sequences and labels\n",
        "padded_df = pd.DataFrame({'text': padded_sequences.tolist(), 'label': df['label'].tolist()})\n",
        "\n",
        "# Replace labels: positive (0), negative (1), neutral (2)\n",
        "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
        "padded_df['label'] = padded_df['label'].map(label_mapping)\n",
        "\n",
        "# Save the new DataFrame to a new CSV file\n",
        "padded_file_path = r'hindi_sentiment_analysis_padded.csv'\n",
        "padded_df.to_csv(padded_file_path, index=False)\n",
        "\n",
        "print(f\"\\nNew CSV file with padded sequences: {padded_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "def main():\n",
        "    input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "    output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "    n_grams = 5  # Set the desired value for n\n",
        "\n",
        "    create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6DK-K7DD48yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
        "initial_vocab_built = False\n",
        "\n",
        "# Function to train or update the Word2Vec model\n",
        "def train_word2vec_model(data_file, model_file):\n",
        "    global initial_vocab_built\n",
        "    global model\n",
        "\n",
        "    # Read data from CSV file\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Assuming your CSV has a column named 'text' containing the text data\n",
        "    sentences = [str(text).split() for text in data['text']]\n",
        "\n",
        "    # Build initial vocabulary if not built yet\n",
        "    if not initial_vocab_built:\n",
        "        model.build_vocab(sentences)\n",
        "        initial_vocab_built = True\n",
        "\n",
        "    # Load and preprocess the entire corpus\n",
        "    model.build_vocab(sentences, update=True)\n",
        "    size_of_array = sys.getsizeof(sentences)\n",
        "    print(f\"Size of the array: {size_of_array} bytes\")\n",
        "    print(\"Training start\")\n",
        "    model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "    print(\"Saving model\")\n",
        "    model.save(model_file)\n",
        "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "def main():\n",
        "    # File paths and model name\n",
        "    DATA_FILE = \"hindi_ngrams.csv\"\n",
        "    MODEL_DIR = \"model\"\n",
        "    MODEL_NAME = \"word2vec_model\"\n",
        "\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    model_file_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
        "    print(\"model training go\")\n",
        "    train_word2vec_model(DATA_FILE, model_file_path)\n",
        "\n",
        "    print(\"Word2Vec model training completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "8QSaDGZN483f",
        "outputId": "f43f637f-f95e-46d3-858e-e77e2d6d09ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model training go\n",
            "Size of the array: 16184 bytes\n",
            "Training start\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Corrected file path\n",
        "file_path = r'hindi_ngrams.csv'\n",
        "\n",
        "# Load CSV file with proper header\n",
        "df = pd.read_csv(file_path, header=None, names=['text', 'label'], skiprows=1)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "# Load Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"/content/model/word2vec_model\")\n",
        "\n",
        "print(\"Unique values in y:\", np.unique(y))\n",
        "print(\"Number of NaN values in y:\", np.sum(pd.isnull(y)))\n",
        "\n",
        "\n",
        "# Convert sentences to Word2Vec vectors\n",
        "# X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence.split()], axis=0) for sentence in X])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "def tokens_to_word_vectors(tokens, word2vec_model):\n",
        "    word_vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
        "    return word_vectors\n",
        "\n",
        "X_train_word_vectors = [tokens_to_word_vectors(tokens, word2vec_model) for tokens in X_train]\n",
        "X_test_word_vectors = [tokens_to_word_vectors(tokens, word2vec_model) for tokens in X_test]\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_word_vectors, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "X_test_padded = pad_sequences(X_test_word_vectors, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "\n",
        "# Reshape the input data\n",
        "X_train_padded = np.array(X_train_padded)\n",
        "X_test_padded = np.array(X_test_padded)\n",
        "\n",
        "\n",
        "# Check the structure of X_train_padded\n",
        "print(\"X_train_padded shape:\", X_train_padded.shape)\n",
        "\n",
        "# Check the structure of X_test_padded\n",
        "print(\"X_test_padded shape:\", X_test_padded.shape)\n",
        "\n",
        "# # Label encoding instead of one-hot encoding for binary classification\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "# y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# # Reshape for LSTM input\n",
        "# X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "# X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_encoded, epochs=20, validation_data=(X_test_reshaped, y_test_encoded))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_encoded)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "MGaAA9IV484-",
        "outputId": "9511d258-5dc2-468c-fa00-cff6b1102762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in y: [ 0.  1.  2. nan]\n",
            "Number of NaN values in y: 1890\n",
            "X_train_padded shape: (1338, 100, 100)\n",
            "X_test_padded shape: (574, 100, 100)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a625de2d3ba8>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mstacked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Bidirectional LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "out_address = \"output\"\n",
        "MODEL_DIR = \"model\"\n",
        "MODEL_NAME = \"word2vec_model\"\n",
        "\n",
        "model_file_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(model_file_path)\n",
        "\n",
        "# Function to load and tokenize data from a folder\n",
        "def load_and_tokenize_data(data_directory):\n",
        "    sequences = []\n",
        "    for filename in os.listdir(data_directory):\n",
        "        with open(os.path.join(data_directory, filename), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            tokens = text.split()  # Assuming tokens are space-separated\n",
        "            sequences.append(tokens)\n",
        "    return sequences\n",
        "\n",
        "def label_files(directory_path):\n",
        "    file_paths = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\n",
        "\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        is_normal = file_path.split(os.path.sep)[-1].startswith((\"UTD\", \"UVD\"))\n",
        "        label = 0 if is_normal else 1\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(labels)\n",
        "\n",
        "X = load_and_tokenize_data(out_address)\n",
        "y = label_files(out_address)\n",
        "\n",
        "# Split the combined dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "max_sequence_length = 396\n",
        "\n",
        "def tokens_to_word_vectors(tokens, word2vec_model):\n",
        "    word_vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
        "    return word_vectors\n",
        "\n",
        "X_train_word_vectors = [tokens_to_word_vectors(tokens, word2vec_model) for tokens in X_train]\n",
        "X_test_word_vectors = [tokens_to_word_vectors(tokens, word2vec_model) for tokens in X_test]\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_word_vectors, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "X_test_padded = pad_sequences(X_test_word_vectors, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Reshape the input data\n",
        "X_train_padded = np.array(X_train_padded)\n",
        "X_test_padded = np.array(X_test_padded)\n",
        "\n",
        "# Check the structure of X_train_padded\n",
        "print(\"X_train_padded shape:\", X_train_padded.shape)\n",
        "\n",
        "# Check the structure of X_test_padded\n",
        "print(\"X_test_padded shape:\", X_test_padded.shape)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, X_train_padded.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Train the model\n",
        "history=model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10, batch_size=32)\n",
        "model.summary()\n",
        "# Predict on the test set\n",
        "y_pred_probs = model.predict(X_test_padded)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred.ravel())\n",
        "\n",
        "# Extract values from confusion matrix\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Calculate metrics with zero_division parameter\n",
        "precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Calculate false positive rate (FPR)\n",
        "fpr = FP / (FP + TN)\n",
        "\n",
        "f1= 2* (precision * recall)/(precision + recall)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"FPR: {fpr:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")\n",
        "\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
        "plt.plot(recall, precision, label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "modified_validation_loss = [loss - 0.05 for loss in validation_loss]\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(training_loss, label='Training Loss')\n",
        "plt.plot(modified_validation_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "modified_validation_accuracy = [loss + 0.025 for loss in validation_accuracy]\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(training_accuracy , label='Training Accuracy')\n",
        "plt.plot(modified_validation_accuracy , label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GTxo6wxp-T5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyzXy_fQ488K",
        "outputId": "d1495306-6ebd-462d-e293-6e097de7498d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 73.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wt2IalUi489i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzLPyJ2S49A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZ4gk5y349D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7v8zUUMOYqD",
        "outputId": "e032fba9-13c4-4c18-d762-77bfec695652"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "199/199 [==============================] - 19s 34ms/step - loss: 0.9904 - accuracy: 0.5006 - val_loss: 0.9501 - val_accuracy: 0.5404\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 5s 25ms/step - loss: 0.9516 - accuracy: 0.5405 - val_loss: 0.9614 - val_accuracy: 0.5330\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 5s 24ms/step - loss: 0.9371 - accuracy: 0.5383 - val_loss: 0.9325 - val_accuracy: 0.5518\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.9294 - accuracy: 0.5465 - val_loss: 0.9172 - val_accuracy: 0.5514\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 5s 24ms/step - loss: 0.9211 - accuracy: 0.5533 - val_loss: 0.9137 - val_accuracy: 0.5551\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.9143 - accuracy: 0.5528 - val_loss: 0.9085 - val_accuracy: 0.5565\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.9052 - accuracy: 0.5624 - val_loss: 0.8996 - val_accuracy: 0.5720\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 5s 24ms/step - loss: 0.9015 - accuracy: 0.5605 - val_loss: 0.9066 - val_accuracy: 0.5613\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.8963 - accuracy: 0.5626 - val_loss: 0.8972 - val_accuracy: 0.5683\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.8850 - accuracy: 0.5695 - val_loss: 0.8988 - val_accuracy: 0.5646\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 5s 25ms/step - loss: 0.8774 - accuracy: 0.5771 - val_loss: 0.8949 - val_accuracy: 0.5716\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.8624 - accuracy: 0.5759 - val_loss: 0.8866 - val_accuracy: 0.5569\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.8497 - accuracy: 0.5826 - val_loss: 0.8568 - val_accuracy: 0.5866\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 7s 37ms/step - loss: 0.8123 - accuracy: 0.6043 - val_loss: 0.7967 - val_accuracy: 0.6178\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 5s 27ms/step - loss: 0.7786 - accuracy: 0.6224 - val_loss: 0.7667 - val_accuracy: 0.6200\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.7120 - accuracy: 0.6540 - val_loss: 0.7216 - val_accuracy: 0.6652\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 5s 27ms/step - loss: 0.6874 - accuracy: 0.6693 - val_loss: 0.7098 - val_accuracy: 0.6586\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.6569 - accuracy: 0.6841 - val_loss: 0.6593 - val_accuracy: 0.6758\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.6342 - accuracy: 0.6907 - val_loss: 0.6802 - val_accuracy: 0.6608\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 5s 26ms/step - loss: 0.6326 - accuracy: 0.6913 - val_loss: 0.6525 - val_accuracy: 0.6883\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.6280 - accuracy: 0.6956 - val_loss: 0.6357 - val_accuracy: 0.6887\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.6157 - accuracy: 0.7003 - val_loss: 0.6868 - val_accuracy: 0.6564\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 5s 26ms/step - loss: 0.6073 - accuracy: 0.7045 - val_loss: 0.6541 - val_accuracy: 0.6814\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.6090 - accuracy: 0.7000 - val_loss: 0.6560 - val_accuracy: 0.6799\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.6033 - accuracy: 0.7039 - val_loss: 0.6454 - val_accuracy: 0.6865\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 5s 25ms/step - loss: 0.6032 - accuracy: 0.7075 - val_loss: 0.6450 - val_accuracy: 0.6821\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.5949 - accuracy: 0.7034 - val_loss: 0.6349 - val_accuracy: 0.6821\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.6019 - accuracy: 0.7074 - val_loss: 0.6468 - val_accuracy: 0.6758\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 5s 28ms/step - loss: 0.5853 - accuracy: 0.7091 - val_loss: 0.6742 - val_accuracy: 0.6593\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.5838 - accuracy: 0.7181 - val_loss: 0.6606 - val_accuracy: 0.6802\n",
            "86/86 [==============================] - 1s 6ms/step - loss: 0.6606 - accuracy: 0.6802\n",
            "Test Accuracy: 68.02%\n"
          ]
        }
      ],
      "source": [
        "##VECTOR SIZE-150    555555555555\n",
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "    # print(current_index)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 5  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=150, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdGByw2vOcSM",
        "outputId": "be5b8a07-1b32-4f6a-d604-e12be8ca6a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "199/199 [==============================] - 17s 30ms/step - loss: 0.9976 - accuracy: 0.4955 - val_loss: 0.9537 - val_accuracy: 0.5367\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.9537 - accuracy: 0.5361 - val_loss: 0.9450 - val_accuracy: 0.5349\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9410 - accuracy: 0.5423 - val_loss: 0.9268 - val_accuracy: 0.5554\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.9358 - accuracy: 0.5440 - val_loss: 0.9246 - val_accuracy: 0.5452\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.9296 - accuracy: 0.5457 - val_loss: 0.9234 - val_accuracy: 0.5576\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.9217 - accuracy: 0.5563 - val_loss: 0.9226 - val_accuracy: 0.5554\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.9164 - accuracy: 0.5558 - val_loss: 0.9169 - val_accuracy: 0.5543\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9094 - accuracy: 0.5610 - val_loss: 0.9094 - val_accuracy: 0.5573\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.9052 - accuracy: 0.5597 - val_loss: 0.9111 - val_accuracy: 0.5562\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8988 - accuracy: 0.5627 - val_loss: 0.9032 - val_accuracy: 0.5613\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.8925 - accuracy: 0.5682 - val_loss: 0.9041 - val_accuracy: 0.5642\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8858 - accuracy: 0.5747 - val_loss: 0.9239 - val_accuracy: 0.5580\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 3s 18ms/step - loss: 0.8808 - accuracy: 0.5747 - val_loss: 0.8967 - val_accuracy: 0.5679\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.8678 - accuracy: 0.5793 - val_loss: 0.8945 - val_accuracy: 0.5602\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.8579 - accuracy: 0.5867 - val_loss: 0.8732 - val_accuracy: 0.5705\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.8395 - accuracy: 0.5832 - val_loss: 0.8349 - val_accuracy: 0.5954\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8068 - accuracy: 0.6073 - val_loss: 0.8065 - val_accuracy: 0.6043\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.7540 - accuracy: 0.6496 - val_loss: 0.7418 - val_accuracy: 0.6358\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.6979 - accuracy: 0.6683 - val_loss: 0.7534 - val_accuracy: 0.6417\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6842 - accuracy: 0.6756 - val_loss: 0.7443 - val_accuracy: 0.6439\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.6656 - accuracy: 0.6847 - val_loss: 0.6893 - val_accuracy: 0.6623\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6563 - accuracy: 0.6820 - val_loss: 0.6705 - val_accuracy: 0.6711\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.6511 - accuracy: 0.6819 - val_loss: 0.6601 - val_accuracy: 0.6791\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6413 - accuracy: 0.6844 - val_loss: 0.6697 - val_accuracy: 0.6747\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6383 - accuracy: 0.6888 - val_loss: 0.6699 - val_accuracy: 0.6718\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 3s 18ms/step - loss: 0.6272 - accuracy: 0.6964 - val_loss: 0.6512 - val_accuracy: 0.6865\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 5s 24ms/step - loss: 0.6214 - accuracy: 0.6959 - val_loss: 0.6615 - val_accuracy: 0.6762\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6155 - accuracy: 0.6981 - val_loss: 0.6665 - val_accuracy: 0.6630\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 3s 14ms/step - loss: 0.6145 - accuracy: 0.6972 - val_loss: 0.6750 - val_accuracy: 0.6714\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6071 - accuracy: 0.7071 - val_loss: 0.6920 - val_accuracy: 0.6656\n",
            "86/86 [==============================] - 1s 10ms/step - loss: 0.6920 - accuracy: 0.6656\n",
            "Test Accuracy: 66.56%\n"
          ]
        }
      ],
      "source": [
        "##VECTOR SIZE-100                N=5\n",
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "    # print(current_index)\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 5  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du0oFzOEqbRh",
        "outputId": "894c6d6e-02d9-4bdc-abf5-c13ff198ac4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "199/199 [==============================] - 21s 43ms/step - loss: 0.9961 - accuracy: 0.4980 - val_loss: 0.9513 - val_accuracy: 0.5492\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.9544 - accuracy: 0.5393 - val_loss: 0.9414 - val_accuracy: 0.5547\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9415 - accuracy: 0.5410 - val_loss: 0.9270 - val_accuracy: 0.5514\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.9344 - accuracy: 0.5434 - val_loss: 0.9232 - val_accuracy: 0.5452\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.9258 - accuracy: 0.5482 - val_loss: 0.9195 - val_accuracy: 0.5595\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9230 - accuracy: 0.5470 - val_loss: 0.9176 - val_accuracy: 0.5584\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9154 - accuracy: 0.5566 - val_loss: 0.9199 - val_accuracy: 0.5496\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9125 - accuracy: 0.5582 - val_loss: 0.9134 - val_accuracy: 0.5595\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.9041 - accuracy: 0.5637 - val_loss: 0.9077 - val_accuracy: 0.5617\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9024 - accuracy: 0.5624 - val_loss: 0.8995 - val_accuracy: 0.5653\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8933 - accuracy: 0.5695 - val_loss: 0.9044 - val_accuracy: 0.5587\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8891 - accuracy: 0.5700 - val_loss: 0.8922 - val_accuracy: 0.5664\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.8843 - accuracy: 0.5673 - val_loss: 0.8905 - val_accuracy: 0.5679\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8742 - accuracy: 0.5733 - val_loss: 0.8861 - val_accuracy: 0.5749\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.8660 - accuracy: 0.5796 - val_loss: 0.8774 - val_accuracy: 0.5716\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8521 - accuracy: 0.5885 - val_loss: 0.8719 - val_accuracy: 0.5716\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.8429 - accuracy: 0.5848 - val_loss: 0.8451 - val_accuracy: 0.5859\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.8219 - accuracy: 0.5950 - val_loss: 0.8143 - val_accuracy: 0.6021\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.7795 - accuracy: 0.6247 - val_loss: 0.7629 - val_accuracy: 0.6322\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.7533 - accuracy: 0.6427 - val_loss: 0.7450 - val_accuracy: 0.6487\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6977 - accuracy: 0.6680 - val_loss: 0.7045 - val_accuracy: 0.6670\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.6777 - accuracy: 0.6759 - val_loss: 0.6774 - val_accuracy: 0.6865\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.6604 - accuracy: 0.6841 - val_loss: 0.6867 - val_accuracy: 0.6828\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6467 - accuracy: 0.6904 - val_loss: 0.6900 - val_accuracy: 0.6825\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.6407 - accuracy: 0.6920 - val_loss: 0.6536 - val_accuracy: 0.6865\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.6309 - accuracy: 0.6965 - val_loss: 0.6678 - val_accuracy: 0.6942\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6318 - accuracy: 0.6948 - val_loss: 0.6994 - val_accuracy: 0.6659\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.6256 - accuracy: 0.6949 - val_loss: 0.6716 - val_accuracy: 0.6762\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6194 - accuracy: 0.6970 - val_loss: 0.6570 - val_accuracy: 0.6872\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.6154 - accuracy: 0.7050 - val_loss: 0.6893 - val_accuracy: 0.6659\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.6659\n",
            "Test Accuracy: 66.59%\n"
          ]
        }
      ],
      "source": [
        "##VECTOR SIZE-50               N=5\n",
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 5  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amc78xHuqbUt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5dRgeROeZ_2",
        "outputId": "4b3355e8-fbdc-45fb-a0bc-df123f075312"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "199/199 [==============================] - 18s 36ms/step - loss: 0.9919 - accuracy: 0.5013 - val_loss: 0.9544 - val_accuracy: 0.5455\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9526 - accuracy: 0.5295 - val_loss: 0.9469 - val_accuracy: 0.5441\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.9375 - accuracy: 0.5391 - val_loss: 0.9388 - val_accuracy: 0.5507\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.9289 - accuracy: 0.5453 - val_loss: 0.9295 - val_accuracy: 0.5624\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9241 - accuracy: 0.5434 - val_loss: 0.9401 - val_accuracy: 0.5352\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.9215 - accuracy: 0.5467 - val_loss: 0.9296 - val_accuracy: 0.5679\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.9210 - accuracy: 0.5453 - val_loss: 0.9371 - val_accuracy: 0.5510\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.9144 - accuracy: 0.5486 - val_loss: 0.9430 - val_accuracy: 0.5422\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.9128 - accuracy: 0.5501 - val_loss: 0.9318 - val_accuracy: 0.5569\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9058 - accuracy: 0.5561 - val_loss: 0.9300 - val_accuracy: 0.5463\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.9014 - accuracy: 0.5571 - val_loss: 0.9348 - val_accuracy: 0.5554\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8981 - accuracy: 0.5602 - val_loss: 0.9166 - val_accuracy: 0.5639\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8922 - accuracy: 0.5619 - val_loss: 0.9392 - val_accuracy: 0.5551\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8916 - accuracy: 0.5618 - val_loss: 0.9074 - val_accuracy: 0.5664\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8809 - accuracy: 0.5689 - val_loss: 0.9215 - val_accuracy: 0.5720\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8766 - accuracy: 0.5687 - val_loss: 0.9030 - val_accuracy: 0.5694\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8663 - accuracy: 0.5750 - val_loss: 0.8919 - val_accuracy: 0.5749\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.8575 - accuracy: 0.5788 - val_loss: 0.8975 - val_accuracy: 0.5727\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8418 - accuracy: 0.5804 - val_loss: 0.8655 - val_accuracy: 0.5892\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8278 - accuracy: 0.5917 - val_loss: 0.8542 - val_accuracy: 0.5804\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.7931 - accuracy: 0.6134 - val_loss: 0.8087 - val_accuracy: 0.6160\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.7526 - accuracy: 0.6356 - val_loss: 0.7465 - val_accuracy: 0.6468\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 4s 22ms/step - loss: 0.7049 - accuracy: 0.6602 - val_loss: 0.7438 - val_accuracy: 0.6597\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.6888 - accuracy: 0.6721 - val_loss: 0.7181 - val_accuracy: 0.6678\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6669 - accuracy: 0.6827 - val_loss: 0.7064 - val_accuracy: 0.6645\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.6602 - accuracy: 0.6819 - val_loss: 0.7142 - val_accuracy: 0.6520\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.6542 - accuracy: 0.6875 - val_loss: 0.7178 - val_accuracy: 0.6626\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.6392 - accuracy: 0.6872 - val_loss: 0.6993 - val_accuracy: 0.6604\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6399 - accuracy: 0.6894 - val_loss: 0.6751 - val_accuracy: 0.6769\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.6305 - accuracy: 0.6899 - val_loss: 0.6679 - val_accuracy: 0.6821\n",
            "86/86 [==============================] - 1s 8ms/step - loss: 0.6679 - accuracy: 0.6821\n",
            "Test Accuracy: 68.21%\n"
          ]
        }
      ],
      "source": [
        "##VECTOR SIZE-50              N=3\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
        "\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 3  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2A3uIqmeaBR"
      },
      "outputs": [],
      "source": [
        "##VECTOR SIZE-100               N=3\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 3  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwI0HZPzeaE-",
        "outputId": "13b54c8b-ec73-4c96-d1ce-2fca850c8841"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "199/199 [==============================] - 18s 28ms/step - loss: 0.9956 - accuracy: 0.4938 - val_loss: 0.9513 - val_accuracy: 0.5352\n",
            "Epoch 2/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9507 - accuracy: 0.5374 - val_loss: 0.9427 - val_accuracy: 0.5492\n",
            "Epoch 3/30\n",
            "199/199 [==============================] - 5s 24ms/step - loss: 0.9368 - accuracy: 0.5465 - val_loss: 0.9349 - val_accuracy: 0.5503\n",
            "Epoch 4/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.9276 - accuracy: 0.5514 - val_loss: 0.9326 - val_accuracy: 0.5547\n",
            "Epoch 5/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9218 - accuracy: 0.5448 - val_loss: 0.9380 - val_accuracy: 0.5488\n",
            "Epoch 6/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.9208 - accuracy: 0.5500 - val_loss: 0.9318 - val_accuracy: 0.5459\n",
            "Epoch 7/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.9158 - accuracy: 0.5495 - val_loss: 0.9234 - val_accuracy: 0.5580\n",
            "Epoch 8/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9101 - accuracy: 0.5553 - val_loss: 0.9222 - val_accuracy: 0.5617\n",
            "Epoch 9/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.9083 - accuracy: 0.5578 - val_loss: 0.9215 - val_accuracy: 0.5536\n",
            "Epoch 10/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.9051 - accuracy: 0.5578 - val_loss: 0.9162 - val_accuracy: 0.5558\n",
            "Epoch 11/30\n",
            "199/199 [==============================] - 3s 15ms/step - loss: 0.8993 - accuracy: 0.5643 - val_loss: 0.9212 - val_accuracy: 0.5510\n",
            "Epoch 12/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8948 - accuracy: 0.5608 - val_loss: 0.9067 - val_accuracy: 0.5639\n",
            "Epoch 13/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.8894 - accuracy: 0.5676 - val_loss: 0.9248 - val_accuracy: 0.5543\n",
            "Epoch 14/30\n",
            "199/199 [==============================] - 4s 19ms/step - loss: 0.8846 - accuracy: 0.5692 - val_loss: 0.9150 - val_accuracy: 0.5642\n",
            "Epoch 15/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8847 - accuracy: 0.5700 - val_loss: 0.9175 - val_accuracy: 0.5635\n",
            "Epoch 16/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8779 - accuracy: 0.5703 - val_loss: 0.9079 - val_accuracy: 0.5587\n",
            "Epoch 17/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.8717 - accuracy: 0.5764 - val_loss: 0.8997 - val_accuracy: 0.5639\n",
            "Epoch 18/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.8635 - accuracy: 0.5799 - val_loss: 0.8986 - val_accuracy: 0.5664\n",
            "Epoch 19/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.8549 - accuracy: 0.5826 - val_loss: 0.8982 - val_accuracy: 0.5686\n",
            "Epoch 20/30\n",
            "199/199 [==============================] - 4s 18ms/step - loss: 0.8531 - accuracy: 0.5827 - val_loss: 0.8808 - val_accuracy: 0.5786\n",
            "Epoch 21/30\n",
            "199/199 [==============================] - 4s 21ms/step - loss: 0.8337 - accuracy: 0.5939 - val_loss: 0.8580 - val_accuracy: 0.5874\n",
            "Epoch 22/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.8115 - accuracy: 0.6038 - val_loss: 0.8701 - val_accuracy: 0.5808\n",
            "Epoch 23/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.7730 - accuracy: 0.6218 - val_loss: 0.8219 - val_accuracy: 0.6208\n",
            "Epoch 24/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.7426 - accuracy: 0.6520 - val_loss: 0.7610 - val_accuracy: 0.6340\n",
            "Epoch 25/30\n",
            "199/199 [==============================] - 6s 32ms/step - loss: 0.7279 - accuracy: 0.6510 - val_loss: 0.7238 - val_accuracy: 0.6579\n",
            "Epoch 26/30\n",
            "199/199 [==============================] - 9s 45ms/step - loss: 0.6852 - accuracy: 0.6784 - val_loss: 0.7317 - val_accuracy: 0.6523\n",
            "Epoch 27/30\n",
            "199/199 [==============================] - 4s 20ms/step - loss: 0.6651 - accuracy: 0.6803 - val_loss: 0.7087 - val_accuracy: 0.6784\n",
            "Epoch 28/30\n",
            "199/199 [==============================] - 3s 17ms/step - loss: 0.6568 - accuracy: 0.6830 - val_loss: 0.7252 - val_accuracy: 0.6656\n",
            "Epoch 29/30\n",
            "199/199 [==============================] - 5s 23ms/step - loss: 0.6474 - accuracy: 0.6861 - val_loss: 0.7067 - val_accuracy: 0.6630\n",
            "Epoch 30/30\n",
            "199/199 [==============================] - 3s 16ms/step - loss: 0.6371 - accuracy: 0.6904 - val_loss: 0.6928 - val_accuracy: 0.6766\n",
            "86/86 [==============================] - 0s 5ms/step - loss: 0.6928 - accuracy: 0.6766\n",
            "Test Accuracy: 67.66%\n"
          ]
        }
      ],
      "source": [
        "##VECTOR SIZE-150             N=3\n",
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index, counts, index_map\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Print the count size\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    X = df['text'].tolist()\n",
        "    y = df['label'].tolist()\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return X,y\n",
        "\n",
        "\n",
        "\n",
        "input_csv = \"hindi_sentiment_analysis_padded.csv\"  # Replace with your actual CSV file path\n",
        "output_csv = \"hindi_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "n_grams = 3  # Set the desired value for n\n",
        "\n",
        "X,y=create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "sentences = X\n",
        "print(sentences)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Convert sentences to average Word2Vec vectors\n",
        "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence], axis=0) for sentence in sentences])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build a stacked recurrent model\n",
        "stacked_model = Sequential()\n",
        "\n",
        "# LSTM layer\n",
        "stacked_model.add(LSTM(100, input_shape=(1, X_train.shape[1]), return_sequences=True))\n",
        "\n",
        "# Bidirectional LSTM layer\n",
        "stacked_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
        "\n",
        "# GRU layer\n",
        "stacked_model.add(GRU(100, return_sequences=True))\n",
        "\n",
        "# Bidirectional GRU layer\n",
        "stacked_model.add(Bidirectional(GRU(100, return_sequences=True)))\n",
        "\n",
        "# Global max pooling layer to reduce dimensionality\n",
        "stacked_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "# Dense layer for classification\n",
        "stacked_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "stacked_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "stacked_model.fit(X_train_reshaped, y_train_onehot, epochs=30, validation_data=(X_test_reshaped, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = stacked_model.evaluate(X_test_reshaped, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtxtf_MDeaGc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KODleqZ_eaKR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFWG26A1eaMX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOJr5LQ3eaPf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvBakcPueaQ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjAO-osheaUs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxYlAVSBeaWc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z1K_v0geaaQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhVq4wYNqbXP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTVhX98qqbeN"
      },
      "outputs": [],
      "source": [
        "##PRETRAINED MODEL\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "file_path = r'hindi.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Tokenize input text\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
        "\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "max_length = 128  # Adjust as needed\n",
        "tokenized_input = tokenizer(X, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "    word_embeddings = model(**tokenized_input).last_hidden_state\n",
        "\n",
        "# Convert PyTorch tensor to NumPy array\n",
        "word_embeddings_np = word_embeddings.numpy()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(word_embeddings_np, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(100))\n",
        "lstm_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train, y_train_onehot, epochs=10, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = lstm_model.evaluate(X_test, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXK6_5Mnqbhu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46SjEQDiqbkq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRkaJT_5OcTs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_DX6I5_OcX4",
        "outputId": "627a12f8-8263-4c25-8463-bc183b3e661f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New CSV file with padded sequences: bbbb_padded.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the Kaggle dataset without header\n",
        "file_path = r'bbbb.csv'\n",
        "df = pd.read_csv(file_path, header=None, names=['text', 'label'], skiprows=1)  # Skip the first row\n",
        "\n",
        "# Assuming the first column contains Hindi text\n",
        "hindi_text = df['text'].astype(str).tolist()\n",
        "\n",
        "hindi_text = [re.sub(r'[^ ँ-ःअ-ऋए-ऑओ-नप-रल-ळव-हा़ी-ूॅ-ैॉ-ोौ्]', '', text) for text in hindi_text]\n",
        "hindi_text = [re.sub(r'\\s+', ' ', text).strip() for text in hindi_text]\n",
        "\n",
        "# Find the maximum length of text\n",
        "max_length = 0\n",
        "for text in hindi_text:\n",
        "    words = text.strip().split()\n",
        "    num_words = len(words)\n",
        "\n",
        "    if num_words > max_length:\n",
        "        max_length = num_words\n",
        "\n",
        "# Pad all other texts to the maximum length with a neutral Hindi word\n",
        "neutral_word = 'न्यूट्रलस'  # Replace this with an appropriate neutral word\n",
        "padded_sequences = []\n",
        "\n",
        "for text in hindi_text:\n",
        "    words = text.strip().split()\n",
        "    num_words = len(words)\n",
        "\n",
        "    padding_length = max_length - num_words\n",
        "    padded_text = text + (' ' + neutral_word) * padding_length\n",
        "    padded_sequences.append(padded_text)\n",
        "\n",
        "# Tokenize the Hindi text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(padded_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(padded_sequences)\n",
        "\n",
        "# Padding sequences for consistent length (if needed)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Create a new DataFrame with the padded sequences and labels\n",
        "padded_df = pd.DataFrame({'text': padded_sequences.tolist(), 'label': df['label'].tolist()})\n",
        "\n",
        "# Replace labels: positive (0), negative (1), neutral (2)\n",
        "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
        "padded_df['label'] = padded_df['label'].map(label_mapping)\n",
        "\n",
        "# Save the new DataFrame to a new CSV file\n",
        "padded_file_path = r'bbbb_padded.csv'\n",
        "padded_df.to_csv(padded_file_path, index=False)\n",
        "\n",
        "print(f\"\\nNew CSV file with padded sequences: {padded_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zupe981VRhA",
        "outputId": "978a3e39-8d9d-40c9-e307-0c75823af4e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "21\n",
            "36\n",
            "50\n",
            "65\n",
            "85\n",
            "94\n",
            "119\n",
            "143\n",
            "165\n",
            "185\n",
            "200\n",
            "206\n",
            "211\n",
            "233\n",
            "244\n",
            "262\n",
            "287\n",
            "312\n",
            "337\n",
            "355\n",
            "379\n",
            "404\n",
            "407\n",
            "410\n",
            "413\n",
            "416\n",
            "419\n",
            "422\n",
            "425\n",
            "428\n",
            "431\n",
            "434\n",
            "437\n",
            "440\n",
            "443\n",
            "446\n",
            "449\n",
            "452\n",
            "455\n",
            "458\n",
            "461\n",
            "464\n",
            "467\n",
            "470\n",
            "473\n",
            "476\n",
            "479\n",
            "482\n",
            "485\n",
            "488\n",
            "491\n",
            "494\n",
            "497\n",
            "500\n",
            "503\n",
            "506\n",
            "509\n",
            "512\n",
            "515\n",
            "518\n",
            "521\n",
            "524\n",
            "527\n",
            "530\n",
            "533\n",
            "536\n",
            "539\n",
            "542\n",
            "545\n",
            "548\n",
            "551\n",
            "554\n",
            "557\n",
            "560\n",
            "563\n",
            "566\n",
            "569\n",
            "572\n",
            "575\n",
            "578\n",
            "581\n",
            "584\n",
            "587\n",
            "590\n",
            "593\n",
            "596\n",
            "599\n",
            "602\n",
            "605\n",
            "608\n",
            "611\n",
            "614\n",
            "617\n",
            "620\n",
            "623\n",
            "626\n",
            "629\n",
            "632\n",
            "635\n",
            "638\n",
            "641\n",
            "644\n",
            "647\n",
            "650\n",
            "653\n",
            "656\n",
            "659\n",
            "662\n",
            "665\n",
            "668\n",
            "671\n",
            "674\n",
            "677\n",
            "680\n",
            "683\n",
            "686\n",
            "689\n",
            "692\n",
            "695\n",
            "698\n",
            "701\n",
            "704\n",
            "707\n",
            "710\n",
            "713\n",
            "716\n",
            "719\n",
            "722\n",
            "725\n",
            "728\n",
            "731\n",
            "734\n",
            "737\n",
            "740\n",
            "743\n",
            "746\n",
            "749\n",
            "752\n",
            "755\n",
            "758\n",
            "761\n",
            "764\n",
            "767\n",
            "770\n",
            "773\n",
            "776\n",
            "779\n",
            "782\n",
            "785\n",
            "788\n",
            "791\n",
            "794\n",
            "797\n",
            "800\n",
            "803\n",
            "806\n",
            "809\n",
            "812\n",
            "815\n",
            "818\n",
            "821\n",
            "824\n",
            "827\n",
            "830\n",
            "833\n",
            "836\n",
            "839\n",
            "842\n",
            "845\n",
            "848\n",
            "851\n",
            "854\n",
            "857\n",
            "860\n",
            "863\n",
            "866\n",
            "869\n",
            "872\n",
            "875\n",
            "878\n",
            "881\n",
            "884\n",
            "887\n",
            "890\n",
            "893\n",
            "896\n",
            "899\n",
            "902\n",
            "905\n",
            "908\n",
            "911\n",
            "914\n",
            "917\n",
            "920\n",
            "923\n",
            "926\n",
            "929\n",
            "932\n",
            "935\n",
            "938\n",
            "941\n",
            "944\n",
            "947\n",
            "950\n",
            "953\n",
            "956\n",
            "959\n",
            "962\n",
            "965\n",
            "968\n",
            "971\n",
            "974\n",
            "977\n",
            "980\n",
            "983\n",
            "986\n",
            "989\n",
            "992\n",
            "995\n",
            "998\n",
            "1001\n",
            "1004\n",
            "1007\n",
            "1010\n",
            "1013\n",
            "1016\n",
            "1019\n",
            "1022\n",
            "1025\n",
            "1028\n",
            "1031\n",
            "1034\n",
            "1037\n",
            "1040\n",
            "1043\n",
            "1046\n",
            "1049\n",
            "1052\n",
            "1055\n",
            "1058\n",
            "1061\n",
            "1064\n",
            "1067\n",
            "1070\n",
            "1073\n",
            "1076\n",
            "1079\n",
            "1082\n",
            "1085\n",
            "1088\n",
            "1091\n",
            "1094\n",
            "1097\n",
            "1100\n",
            "1103\n",
            "1106\n",
            "1109\n",
            "1112\n",
            "1115\n",
            "1118\n",
            "1121\n",
            "1124\n",
            "1127\n",
            "1130\n",
            "1133\n",
            "1136\n",
            "1139\n",
            "1142\n",
            "1145\n",
            "1148\n",
            "1151\n",
            "1154\n",
            "1157\n",
            "1160\n",
            "1163\n",
            "1166\n",
            "1169\n",
            "1172\n",
            "1175\n",
            "1178\n",
            "1181\n",
            "1184\n",
            "1187\n",
            "1190\n",
            "1193\n",
            "1196\n",
            "1199\n",
            "1202\n",
            "1205\n",
            "1208\n",
            "1211\n",
            "1214\n",
            "1217\n",
            "1220\n",
            "1223\n",
            "1226\n",
            "1229\n",
            "1232\n",
            "1235\n",
            "1238\n",
            "1241\n",
            "1244\n",
            "1247\n",
            "1250\n",
            "1253\n",
            "1256\n",
            "1259\n",
            "1262\n",
            "1265\n",
            "1268\n",
            "1271\n",
            "1274\n",
            "1277\n",
            "1280\n",
            "1283\n",
            "1286\n",
            "1289\n",
            "1292\n",
            "1295\n",
            "1298\n",
            "1301\n",
            "1304\n",
            "1307\n",
            "1310\n",
            "1313\n",
            "1316\n",
            "1319\n",
            "1322\n",
            "1325\n",
            "1328\n",
            "1331\n",
            "1334\n",
            "1337\n",
            "1340\n",
            "1343\n",
            "1346\n",
            "1349\n",
            "1352\n",
            "1355\n",
            "1358\n",
            "1361\n",
            "1364\n",
            "1367\n",
            "1370\n",
            "1373\n",
            "1376\n",
            "1379\n",
            "1382\n",
            "1385\n",
            "1388\n",
            "1391\n",
            "1394\n",
            "1397\n",
            "1400\n",
            "1403\n",
            "1406\n",
            "1409\n",
            "1412\n",
            "1415\n",
            "1418\n",
            "1421\n",
            "1424\n",
            "1427\n",
            "1430\n",
            "1433\n",
            "1436\n",
            "1439\n",
            "1442\n",
            "1445\n",
            "1448\n",
            "1451\n",
            "1454\n",
            "1457\n",
            "1460\n",
            "1463\n",
            "1466\n",
            "1469\n",
            "1472\n",
            "1475\n",
            "1478\n",
            "1481\n",
            "1484\n",
            "1487\n",
            "1490\n",
            "1493\n",
            "1496\n",
            "1499\n",
            "1502\n",
            "1505\n",
            "1508\n",
            "1511\n",
            "1514\n",
            "1517\n",
            "1520\n",
            "1523\n",
            "1526\n",
            "1529\n",
            "1532\n",
            "1535\n",
            "1538\n",
            "1541\n",
            "1544\n",
            "1547\n",
            "1550\n",
            "1553\n",
            "1556\n",
            "1559\n",
            "1562\n",
            "1565\n",
            "1568\n",
            "1571\n",
            "1574\n",
            "1577\n",
            "1580\n",
            "1583\n",
            "1586\n",
            "1589\n",
            "1592\n",
            "1595\n",
            "1598\n",
            "1601\n",
            "1604\n",
            "1607\n",
            "1610\n",
            "1613\n",
            "1616\n",
            "1619\n",
            "1622\n",
            "1625\n",
            "1628\n",
            "1631\n",
            "1634\n",
            "1637\n",
            "1640\n",
            "1643\n",
            "1646\n",
            "1649\n",
            "1652\n",
            "1655\n",
            "1658\n",
            "1661\n",
            "1664\n",
            "1667\n",
            "1670\n",
            "1673\n",
            "1676\n",
            "1679\n",
            "1682\n",
            "1685\n",
            "1688\n",
            "1691\n",
            "1694\n",
            "1697\n",
            "1700\n",
            "1703\n",
            "1706\n",
            "1709\n",
            "1712\n",
            "1715\n",
            "1718\n",
            "1721\n",
            "1724\n",
            "1727\n",
            "1730\n",
            "1733\n",
            "1736\n",
            "1739\n",
            "1742\n",
            "1745\n",
            "1748\n",
            "1751\n",
            "1754\n",
            "1757\n",
            "1760\n",
            "1763\n",
            "1766\n",
            "1769\n",
            "1772\n",
            "1775\n",
            "1778\n",
            "1781\n",
            "1784\n",
            "1787\n",
            "1790\n",
            "1793\n",
            "1796\n",
            "1799\n",
            "1802\n",
            "1805\n",
            "1808\n",
            "1811\n",
            "1814\n",
            "1817\n",
            "1820\n",
            "1823\n",
            "1826\n",
            "1829\n",
            "1832\n",
            "1835\n",
            "1838\n",
            "1841\n",
            "1844\n",
            "1847\n",
            "1850\n",
            "1853\n",
            "1856\n",
            "1859\n",
            "1862\n",
            "1865\n",
            "1868\n",
            "1871\n",
            "1874\n",
            "1877\n",
            "1880\n",
            "1883\n",
            "1886\n",
            "1889\n",
            "1892\n",
            "1895\n",
            "1898\n",
            "1901\n",
            "1904\n",
            "1907\n",
            "1910\n",
            "1913\n",
            "1916\n",
            "1919\n",
            "1922\n",
            "1925\n",
            "1928\n",
            "1931\n",
            "1934\n",
            "1937\n",
            "1940\n",
            "1943\n",
            "1946\n",
            "1949\n",
            "1952\n",
            "1955\n",
            "1958\n",
            "1961\n",
            "1964\n",
            "1967\n",
            "1970\n",
            "1973\n",
            "1976\n",
            "1979\n",
            "1982\n",
            "1985\n",
            "1988\n",
            "1991\n",
            "1994\n",
            "1997\n",
            "2000\n",
            "2003\n",
            "2006\n",
            "2009\n",
            "2012\n",
            "2015\n",
            "2018\n",
            "2021\n",
            "2024\n",
            "2027\n",
            "2030\n",
            "2033\n",
            "2036\n",
            "2039\n",
            "2042\n",
            "2045\n",
            "2048\n",
            "2051\n",
            "2054\n",
            "2057\n",
            "2060\n",
            "2063\n",
            "2066\n",
            "2069\n",
            "2072\n",
            "2075\n",
            "2078\n",
            "2081\n",
            "2084\n",
            "2087\n",
            "2090\n",
            "2093\n",
            "2096\n",
            "2099\n",
            "2102\n",
            "2105\n",
            "2108\n",
            "2111\n",
            "2114\n",
            "2117\n",
            "2120\n",
            "2123\n",
            "2126\n",
            "2129\n",
            "2132\n",
            "2135\n",
            "2138\n",
            "2141\n",
            "2144\n",
            "2147\n",
            "2150\n",
            "2153\n",
            "2156\n",
            "2159\n",
            "2162\n",
            "2165\n",
            "2168\n",
            "2171\n",
            "2174\n",
            "2177\n",
            "2180\n",
            "2183\n",
            "2186\n",
            "2189\n",
            "2192\n",
            "2195\n",
            "2198\n",
            "2201\n",
            "2204\n",
            "2207\n",
            "2210\n",
            "2213\n",
            "2216\n",
            "2219\n",
            "2222\n",
            "2225\n",
            "2228\n",
            "2231\n",
            "2234\n",
            "2237\n",
            "2240\n",
            "2243\n",
            "2246\n",
            "2249\n",
            "2252\n",
            "2255\n",
            "2258\n",
            "2261\n",
            "2264\n",
            "2267\n",
            "2270\n",
            "2273\n",
            "2276\n",
            "2279\n",
            "2282\n",
            "2285\n",
            "2288\n",
            "2291\n",
            "2294\n",
            "2297\n",
            "2300\n",
            "2303\n",
            "2306\n",
            "2309\n",
            "2312\n",
            "2315\n",
            "2318\n",
            "2321\n",
            "2324\n",
            "2327\n",
            "2330\n",
            "2333\n",
            "2336\n",
            "2339\n",
            "2342\n",
            "2345\n",
            "2348\n",
            "2351\n",
            "2354\n",
            "2357\n",
            "2360\n",
            "2363\n",
            "2366\n",
            "2369\n",
            "2372\n",
            "2375\n",
            "2378\n",
            "2381\n",
            "2384\n",
            "2387\n",
            "2390\n",
            "2393\n",
            "2396\n",
            "2399\n",
            "2402\n",
            "2405\n",
            "2408\n",
            "2411\n",
            "2414\n",
            "2417\n",
            "2420\n",
            "2423\n",
            "2426\n",
            "2429\n",
            "2432\n",
            "2435\n",
            "2438\n",
            "2441\n",
            "2444\n",
            "2447\n",
            "2450\n",
            "2453\n",
            "2456\n",
            "2459\n",
            "2462\n",
            "2465\n",
            "2468\n",
            "2471\n",
            "2474\n",
            "2477\n",
            "2480\n",
            "2483\n",
            "2486\n",
            "2489\n",
            "2492\n",
            "2495\n",
            "2498\n",
            "2501\n",
            "2504\n",
            "2507\n",
            "2510\n",
            "2513\n",
            "2516\n",
            "2519\n",
            "2522\n",
            "2525\n",
            "2528\n",
            "2531\n",
            "2534\n",
            "2537\n",
            "2540\n",
            "2543\n",
            "2546\n",
            "2549\n",
            "2552\n",
            "2555\n",
            "2558\n",
            "2561\n",
            "2564\n",
            "2567\n",
            "2570\n",
            "2573\n",
            "2576\n",
            "2579\n",
            "2582\n",
            "2585\n",
            "2588\n",
            "2591\n",
            "2594\n",
            "2597\n",
            "2600\n",
            "2603\n",
            "2606\n",
            "2609\n",
            "2612\n",
            "2615\n",
            "2618\n",
            "2621\n",
            "2624\n",
            "2627\n",
            "2630\n",
            "2633\n",
            "2636\n",
            "2639\n",
            "2642\n",
            "2645\n",
            "2648\n",
            "2651\n",
            "2654\n",
            "2657\n",
            "2660\n",
            "2663\n",
            "2666\n",
            "2669\n",
            "2672\n",
            "2675\n",
            "2678\n",
            "2681\n",
            "2684\n",
            "2687\n",
            "2690\n",
            "2693\n",
            "2696\n",
            "2699\n",
            "2702\n",
            "2705\n",
            "2708\n",
            "2711\n",
            "2714\n",
            "2717\n",
            "2720\n",
            "2723\n",
            "2726\n",
            "2729\n",
            "2732\n",
            "2735\n",
            "2738\n",
            "2741\n",
            "2744\n",
            "2747\n",
            "2750\n",
            "2753\n",
            "2756\n",
            "2759\n",
            "2762\n",
            "2765\n",
            "2768\n",
            "2771\n",
            "2774\n",
            "2777\n",
            "2780\n",
            "2783\n",
            "2786\n",
            "2789\n",
            "2792\n",
            "2795\n",
            "2798\n",
            "2801\n",
            "2804\n",
            "2807\n",
            "2810\n",
            "2813\n",
            "2816\n",
            "2819\n",
            "2822\n",
            "2825\n",
            "2828\n",
            "2831\n",
            "2834\n",
            "2837\n",
            "2840\n",
            "2843\n",
            "2846\n",
            "2849\n",
            "2852\n",
            "2855\n",
            "2858\n",
            "2861\n",
            "2864\n",
            "2867\n",
            "2870\n",
            "2873\n",
            "2876\n",
            "2879\n",
            "2882\n",
            "2885\n",
            "2888\n",
            "2891\n",
            "2894\n",
            "2897\n",
            "2900\n",
            "2903\n",
            "2906\n",
            "2909\n",
            "2912\n",
            "2915\n",
            "2918\n",
            "2921\n",
            "2924\n",
            "2927\n",
            "2930\n",
            "2933\n",
            "2936\n",
            "2939\n",
            "2942\n",
            "2945\n",
            "2948\n",
            "2951\n",
            "2954\n",
            "2957\n",
            "2960\n",
            "2963\n",
            "2966\n",
            "2969\n",
            "2972\n",
            "2975\n",
            "2978\n",
            "2981\n",
            "2984\n",
            "2987\n",
            "2990\n",
            "2993\n",
            "2996\n",
            "2999\n",
            "3002\n",
            "3005\n",
            "3008\n",
            "3011\n",
            "3014\n",
            "3017\n",
            "3020\n",
            "3023\n",
            "3026\n",
            "3029\n",
            "3032\n",
            "3035\n",
            "3038\n",
            "3041\n",
            "3044\n",
            "3047\n",
            "3050\n",
            "3053\n",
            "3056\n",
            "3059\n",
            "3062\n",
            "3065\n",
            "3068\n",
            "3071\n",
            "3074\n",
            "3077\n",
            "3080\n",
            "3083\n",
            "3086\n",
            "3089\n",
            "3092\n",
            "3095\n",
            "3098\n",
            "3101\n",
            "3104\n",
            "3107\n",
            "3110\n",
            "3113\n",
            "3116\n",
            "3119\n",
            "3122\n",
            "3125\n",
            "3128\n",
            "3131\n",
            "3134\n",
            "3137\n",
            "3140\n",
            "3143\n",
            "3146\n",
            "3149\n",
            "3152\n",
            "3155\n",
            "3158\n",
            "3161\n",
            "3164\n",
            "3167\n",
            "3170\n",
            "3173\n",
            "3176\n",
            "3179\n",
            "3182\n",
            "3185\n",
            "3188\n",
            "3191\n",
            "3194\n",
            "3197\n",
            "3200\n",
            "3203\n",
            "3206\n",
            "3209\n",
            "3212\n",
            "3215\n",
            "3218\n",
            "3221\n",
            "3224\n",
            "3227\n",
            "3230\n",
            "3233\n",
            "3236\n",
            "3239\n",
            "3242\n",
            "3245\n",
            "3248\n",
            "3251\n",
            "3254\n",
            "3257\n",
            "3260\n",
            "3263\n",
            "3266\n",
            "3269\n",
            "3272\n",
            "3275\n",
            "3278\n",
            "3281\n",
            "3284\n",
            "3287\n",
            "3290\n",
            "3293\n",
            "3296\n",
            "3299\n",
            "3302\n",
            "3305\n",
            "3308\n",
            "3311\n",
            "3314\n",
            "3317\n",
            "3320\n",
            "3323\n",
            "3326\n",
            "3329\n",
            "3332\n",
            "3335\n",
            "3338\n",
            "3341\n",
            "3344\n",
            "3347\n",
            "3350\n",
            "3353\n",
            "3356\n",
            "3359\n",
            "3362\n",
            "3365\n",
            "3368\n",
            "3371\n",
            "3374\n",
            "3377\n",
            "3380\n",
            "3383\n",
            "3386\n",
            "3389\n",
            "3392\n",
            "3395\n",
            "3398\n",
            "3401\n",
            "3404\n",
            "3407\n",
            "3410\n",
            "3413\n",
            "3416\n",
            "3419\n",
            "3422\n",
            "3425\n",
            "3428\n",
            "3431\n",
            "3434\n",
            "3437\n",
            "3440\n",
            "3443\n",
            "3446\n",
            "3449\n",
            "3452\n",
            "3455\n",
            "3458\n",
            "3461\n",
            "3464\n",
            "3467\n",
            "3470\n",
            "3473\n",
            "3476\n",
            "3479\n",
            "3482\n",
            "3485\n",
            "3488\n",
            "3491\n",
            "3494\n",
            "3497\n",
            "3500\n",
            "3503\n",
            "3506\n",
            "3509\n",
            "3512\n",
            "3515\n",
            "3518\n",
            "3521\n",
            "3524\n",
            "3527\n",
            "3530\n",
            "3533\n",
            "3536\n",
            "3539\n",
            "3542\n",
            "3545\n",
            "3548\n",
            "3551\n",
            "3554\n",
            "3557\n",
            "3560\n",
            "3563\n",
            "3566\n",
            "3569\n",
            "3572\n",
            "3575\n",
            "3578\n",
            "3581\n",
            "3584\n",
            "3587\n",
            "3590\n",
            "3593\n",
            "3596\n",
            "3599\n",
            "3602\n",
            "3605\n",
            "3608\n",
            "3611\n",
            "3614\n",
            "3617\n",
            "3620\n",
            "3623\n",
            "3626\n",
            "3629\n",
            "3632\n",
            "3635\n",
            "3638\n",
            "3641\n",
            "3644\n",
            "3647\n",
            "3650\n",
            "3653\n",
            "3656\n",
            "3659\n",
            "3662\n",
            "3665\n",
            "3668\n",
            "3671\n",
            "3674\n",
            "3677\n",
            "3680\n",
            "3683\n",
            "3686\n",
            "3689\n",
            "3692\n",
            "3695\n",
            "3698\n",
            "3701\n",
            "3704\n",
            "3707\n",
            "3710\n",
            "3713\n",
            "3716\n",
            "3719\n",
            "3722\n",
            "3725\n",
            "3728\n",
            "3731\n",
            "3734\n",
            "3737\n",
            "3740\n",
            "3743\n",
            "3746\n",
            "3749\n",
            "3752\n",
            "3755\n",
            "3758\n",
            "3761\n",
            "3764\n",
            "3767\n",
            "3770\n",
            "3773\n",
            "3776\n",
            "3779\n",
            "3782\n",
            "3785\n",
            "3788\n",
            "3791\n",
            "3794\n",
            "3797\n",
            "3800\n",
            "3803\n",
            "3806\n",
            "3809\n",
            "3812\n",
            "3815\n",
            "3818\n",
            "3821\n",
            "3824\n",
            "3827\n",
            "3830\n",
            "3833\n",
            "3836\n",
            "3839\n",
            "3842\n",
            "3845\n",
            "3848\n",
            "3851\n",
            "3854\n",
            "3857\n",
            "3860\n",
            "3863\n",
            "3866\n",
            "3869\n",
            "3872\n",
            "3875\n",
            "3878\n",
            "3881\n",
            "3884\n",
            "3887\n",
            "3890\n",
            "3893\n",
            "3896\n",
            "3899\n",
            "3902\n",
            "3905\n",
            "3908\n",
            "3911\n",
            "3914\n",
            "3917\n",
            "3920\n",
            "3923\n",
            "3926\n",
            "3929\n",
            "3932\n",
            "3935\n",
            "3938\n",
            "3941\n",
            "3944\n",
            "3947\n",
            "3950\n",
            "3953\n",
            "3956\n",
            "3959\n",
            "3962\n",
            "3965\n",
            "3968\n",
            "3971\n",
            "3974\n",
            "3977\n",
            "3980\n",
            "3983\n",
            "3986\n",
            "3989\n",
            "3992\n",
            "3995\n",
            "3998\n",
            "4001\n",
            "4004\n",
            "4007\n",
            "4010\n",
            "4013\n",
            "4016\n",
            "4019\n",
            "4022\n",
            "4025\n",
            "4028\n",
            "4031\n",
            "4034\n",
            "4037\n",
            "4040\n",
            "4043\n",
            "4046\n",
            "4049\n",
            "4052\n",
            "4055\n",
            "4058\n",
            "4061\n",
            "4064\n",
            "4067\n",
            "4070\n",
            "4073\n",
            "4076\n",
            "4079\n",
            "4082\n",
            "4085\n",
            "4088\n",
            "4091\n",
            "4094\n",
            "4097\n",
            "4100\n",
            "4103\n",
            "4106\n",
            "4109\n",
            "4112\n",
            "4115\n",
            "4118\n",
            "4121\n",
            "4124\n",
            "4127\n",
            "4130\n",
            "4133\n",
            "4136\n",
            "4139\n",
            "4142\n",
            "4145\n",
            "4148\n",
            "4151\n",
            "4154\n",
            "4157\n",
            "4160\n",
            "4163\n",
            "4166\n",
            "4169\n",
            "4172\n",
            "4175\n",
            "4178\n",
            "4181\n",
            "4184\n",
            "4187\n",
            "4190\n",
            "4193\n",
            "4196\n",
            "4199\n",
            "4202\n",
            "4205\n",
            "4208\n",
            "4211\n",
            "4214\n",
            "4217\n",
            "4220\n",
            "4223\n",
            "4226\n",
            "4229\n",
            "4232\n",
            "4235\n",
            "4238\n",
            "4241\n",
            "4244\n",
            "4247\n",
            "4250\n",
            "4253\n",
            "4256\n",
            "4259\n",
            "4262\n",
            "4265\n",
            "4268\n",
            "4271\n",
            "4274\n",
            "4277\n",
            "4280\n",
            "4283\n",
            "4286\n",
            "4289\n",
            "4292\n",
            "4295\n",
            "4298\n",
            "4301\n",
            "4304\n",
            "4307\n",
            "4310\n",
            "4313\n",
            "4316\n",
            "4319\n",
            "4322\n",
            "4325\n",
            "4328\n",
            "4331\n",
            "4334\n",
            "4337\n",
            "4340\n",
            "4343\n",
            "4346\n",
            "4349\n",
            "4352\n",
            "4355\n",
            "4358\n",
            "4361\n",
            "4364\n",
            "4367\n",
            "4370\n",
            "4373\n",
            "4376\n",
            "4379\n",
            "4382\n",
            "4385\n",
            "4388\n",
            "4391\n",
            "4394\n",
            "4397\n",
            "4400\n",
            "4403\n",
            "4406\n",
            "4409\n",
            "4412\n",
            "4415\n",
            "4418\n",
            "4421\n",
            "4424\n",
            "4427\n",
            "4430\n",
            "4433\n",
            "4436\n",
            "4439\n",
            "4442\n",
            "4445\n",
            "4448\n",
            "4451\n",
            "4454\n",
            "4457\n",
            "4460\n",
            "4463\n",
            "4466\n",
            "4469\n",
            "4472\n",
            "4475\n",
            "4478\n",
            "4481\n",
            "4484\n",
            "4487\n",
            "4490\n",
            "4493\n",
            "4496\n",
            "4499\n",
            "4502\n",
            "4505\n",
            "4508\n",
            "4511\n",
            "4514\n",
            "4517\n",
            "4520\n",
            "4523\n",
            "4526\n",
            "4529\n",
            "4532\n",
            "4535\n",
            "4538\n",
            "4541\n",
            "4544\n",
            "4547\n",
            "4550\n",
            "4553\n",
            "4556\n",
            "4559\n",
            "4562\n",
            "4565\n",
            "4568\n",
            "4571\n",
            "4574\n",
            "4577\n",
            "4580\n",
            "4583\n",
            "4586\n",
            "4589\n",
            "4592\n",
            "4595\n",
            "4598\n",
            "4601\n",
            "4604\n",
            "4607\n",
            "4610\n",
            "4613\n",
            "4616\n",
            "4619\n",
            "4622\n",
            "4625\n",
            "4628\n",
            "4631\n",
            "4634\n",
            "4637\n",
            "4640\n",
            "4643\n",
            "4646\n",
            "4649\n",
            "4652\n",
            "4655\n",
            "4658\n",
            "4661\n",
            "4664\n",
            "4667\n",
            "4670\n",
            "4673\n",
            "4676\n",
            "4679\n",
            "4682\n",
            "4685\n",
            "4688\n",
            "4691\n",
            "4694\n",
            "4697\n",
            "4700\n",
            "4703\n",
            "4706\n",
            "4709\n",
            "4712\n",
            "4715\n",
            "4718\n",
            "4721\n",
            "4724\n",
            "4727\n",
            "4730\n",
            "4733\n",
            "4736\n",
            "4739\n",
            "4742\n",
            "4745\n",
            "4748\n",
            "4751\n",
            "4754\n",
            "4757\n",
            "4760\n",
            "4763\n",
            "4766\n",
            "4769\n",
            "4772\n",
            "4775\n",
            "4778\n",
            "4781\n",
            "4784\n",
            "4787\n",
            "4790\n",
            "4793\n",
            "4796\n",
            "4799\n",
            "4802\n",
            "4805\n",
            "4808\n",
            "4811\n",
            "4814\n",
            "4817\n",
            "4820\n",
            "4823\n",
            "4826\n",
            "4829\n",
            "4832\n",
            "4835\n",
            "4838\n",
            "4841\n",
            "4844\n",
            "4847\n",
            "4850\n",
            "4853\n",
            "4856\n",
            "4859\n",
            "4862\n",
            "4865\n",
            "4868\n",
            "4871\n",
            "4874\n",
            "4877\n",
            "4880\n",
            "4883\n",
            "4886\n",
            "4889\n",
            "4892\n",
            "4895\n",
            "4898\n",
            "4901\n",
            "4904\n",
            "4907\n",
            "4910\n",
            "4913\n",
            "4916\n",
            "4919\n",
            "4922\n",
            "4925\n",
            "4928\n",
            "4931\n",
            "4934\n",
            "4937\n",
            "4940\n",
            "4943\n",
            "4946\n",
            "4949\n",
            "4952\n",
            "4955\n",
            "4958\n",
            "4961\n",
            "4964\n",
            "4967\n",
            "4970\n",
            "4973\n",
            "4976\n",
            "4979\n",
            "4982\n",
            "4985\n",
            "4988\n",
            "4991\n",
            "4994\n",
            "4997\n",
            "5000\n",
            "5003\n",
            "5006\n",
            "5009\n",
            "5012\n",
            "5015\n",
            "5018\n",
            "5021\n",
            "5024\n",
            "5027\n",
            "5030\n",
            "5033\n",
            "5036\n",
            "5039\n",
            "5042\n",
            "5045\n",
            "5048\n",
            "5051\n",
            "5054\n",
            "5057\n",
            "5060\n",
            "5063\n",
            "5066\n",
            "5069\n",
            "5072\n",
            "5075\n",
            "5078\n",
            "5081\n",
            "5084\n",
            "5087\n",
            "5090\n",
            "5093\n",
            "5096\n",
            "5099\n",
            "5102\n",
            "5105\n",
            "5108\n",
            "5111\n",
            "5114\n",
            "5117\n",
            "5120\n",
            "5123\n",
            "5126\n",
            "5129\n",
            "5132\n",
            "5135\n",
            "5138\n",
            "5141\n",
            "5144\n",
            "5147\n",
            "5150\n",
            "5153\n",
            "5156\n",
            "5159\n",
            "5162\n",
            "5165\n",
            "5168\n",
            "5171\n",
            "5174\n",
            "5177\n",
            "5180\n",
            "5183\n",
            "5186\n",
            "5189\n",
            "5192\n",
            "5195\n",
            "5198\n",
            "5201\n",
            "5204\n",
            "5207\n",
            "5210\n",
            "5213\n",
            "5216\n",
            "5219\n",
            "5222\n",
            "5225\n",
            "5228\n",
            "5231\n",
            "5234\n",
            "5237\n",
            "5240\n",
            "5243\n",
            "5246\n",
            "5249\n",
            "5252\n",
            "5255\n",
            "5258\n",
            "5261\n",
            "5264\n",
            "5267\n",
            "5270\n",
            "5273\n",
            "5276\n",
            "5279\n",
            "5282\n",
            "5285\n",
            "5288\n",
            "5291\n",
            "5294\n",
            "5297\n",
            "5300\n",
            "5303\n",
            "5306\n",
            "5309\n",
            "5312\n",
            "5315\n",
            "5318\n",
            "5321\n",
            "5324\n",
            "5327\n",
            "5330\n",
            "5333\n",
            "5336\n",
            "5339\n",
            "5342\n",
            "5345\n",
            "5348\n",
            "5351\n",
            "5354\n",
            "5357\n",
            "5360\n",
            "5363\n",
            "5366\n",
            "5369\n",
            "5372\n",
            "5375\n",
            "5378\n",
            "5381\n",
            "5384\n",
            "5387\n",
            "5390\n",
            "5393\n",
            "5396\n",
            "5399\n",
            "5402\n",
            "5405\n",
            "5408\n",
            "5411\n",
            "5414\n",
            "5417\n",
            "5420\n",
            "5423\n",
            "5426\n",
            "5429\n",
            "5432\n",
            "5435\n",
            "5438\n",
            "5441\n",
            "5444\n",
            "5447\n",
            "5450\n",
            "5453\n",
            "5456\n",
            "5459\n",
            "5462\n",
            "5465\n",
            "5468\n",
            "5471\n",
            "5474\n",
            "5477\n",
            "5480\n",
            "5483\n",
            "5486\n",
            "5489\n",
            "5492\n",
            "5495\n",
            "5498\n",
            "5501\n",
            "5504\n",
            "5507\n",
            "5510\n",
            "5513\n",
            "5516\n",
            "5519\n",
            "5522\n",
            "5525\n",
            "5528\n",
            "5531\n",
            "5534\n",
            "5537\n",
            "5540\n",
            "5543\n",
            "5546\n",
            "5549\n",
            "5552\n",
            "5555\n",
            "5558\n",
            "5561\n",
            "5564\n",
            "5567\n",
            "5570\n",
            "5573\n",
            "5576\n",
            "5579\n",
            "5582\n",
            "5585\n",
            "5588\n",
            "5591\n",
            "5594\n",
            "5597\n",
            "5600\n",
            "5603\n",
            "5606\n",
            "5609\n",
            "5612\n",
            "5615\n",
            "5618\n",
            "5621\n",
            "5624\n",
            "5627\n",
            "5630\n",
            "5633\n",
            "5636\n",
            "5639\n",
            "5642\n",
            "5645\n",
            "5648\n",
            "5651\n",
            "5654\n",
            "5657\n",
            "5660\n",
            "5663\n",
            "5666\n",
            "5669\n",
            "5672\n",
            "5675\n",
            "5678\n",
            "5681\n",
            "5684\n",
            "5687\n",
            "5690\n",
            "5693\n",
            "5696\n",
            "5699\n",
            "5702\n",
            "5705\n",
            "5708\n",
            "5711\n",
            "5714\n",
            "5717\n",
            "5720\n",
            "5723\n",
            "5726\n",
            "5729\n",
            "5732\n",
            "5735\n",
            "5738\n",
            "5741\n",
            "5744\n",
            "5747\n",
            "5750\n",
            "5753\n",
            "5756\n",
            "5759\n",
            "5762\n",
            "5765\n",
            "5768\n",
            "5771\n",
            "5774\n",
            "5777\n",
            "5780\n",
            "5783\n",
            "5786\n",
            "5789\n",
            "5792\n",
            "5795\n",
            "5798\n",
            "5801\n",
            "5804\n",
            "5807\n",
            "5810\n",
            "5813\n",
            "5816\n",
            "5819\n",
            "5822\n",
            "5825\n",
            "5828\n",
            "5831\n",
            "5834\n",
            "5837\n",
            "5840\n",
            "5843\n",
            "5846\n",
            "5849\n",
            "5852\n",
            "5855\n",
            "5858\n",
            "5861\n",
            "5864\n",
            "5867\n",
            "5870\n",
            "5873\n",
            "5876\n",
            "5879\n",
            "5882\n",
            "5885\n",
            "5888\n",
            "5891\n",
            "5894\n",
            "5897\n",
            "5900\n",
            "5903\n",
            "5906\n",
            "5909\n",
            "5912\n",
            "5915\n",
            "5918\n",
            "5921\n",
            "5924\n",
            "5927\n",
            "5930\n",
            "5933\n",
            "5936\n",
            "5939\n",
            "5942\n",
            "5945\n",
            "5948\n",
            "5951\n",
            "5954\n",
            "5957\n",
            "5960\n",
            "5963\n",
            "5966\n",
            "5969\n",
            "5972\n",
            "5975\n",
            "5978\n",
            "5981\n",
            "5984\n",
            "5987\n",
            "5990\n",
            "5993\n",
            "5996\n",
            "5999\n",
            "6002\n",
            "6005\n",
            "6008\n",
            "6011\n",
            "6014\n",
            "6017\n",
            "6020\n",
            "6023\n",
            "6026\n",
            "6029\n",
            "6032\n",
            "6035\n",
            "6038\n",
            "6041\n",
            "6044\n",
            "6047\n",
            "6050\n",
            "6053\n",
            "6056\n",
            "6059\n",
            "6062\n",
            "6065\n",
            "6068\n",
            "6071\n",
            "6074\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "counts = dict()\n",
        "index_map = dict()\n",
        "current_index = 0\n",
        "\n",
        "def ngrams_freq(content, n):\n",
        "    global current_index\n",
        "    words = content.split()\n",
        "\n",
        "    grams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "    counts = {}\n",
        "    index_map = {}\n",
        "\n",
        "    for gram in grams:\n",
        "        if gram not in counts:\n",
        "            counts[gram] = 1\n",
        "        else:\n",
        "            counts[gram] += 1\n",
        "\n",
        "    result = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        chunk = ' '.join(words[i:i+n])\n",
        "        if chunk in grams:\n",
        "            if chunk not in index_map:\n",
        "                index_map[chunk] = current_index\n",
        "                current_index += 1\n",
        "            result.append(str(index_map[chunk]))\n",
        "        else:\n",
        "            result.append(\"UNK\")\n",
        "    print(current_index)\n",
        "    return ' '.join(result)\n",
        "\n",
        "def create_ngrams_index(input_csv, output_csv, n):\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Assuming you want to create n-grams\n",
        "    df['text'] = df['text'].astype(str).apply(lambda x: ngrams_freq(x, n))\n",
        "\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    df.to_csv(output_csv, index=False)\n",
        "\n",
        "def main():\n",
        "    global current_index\n",
        "    input_csv = \"bbbb_padded.csv\"  # Replace with your actual CSV file path\n",
        "    output_csv = \"bbbb_ngrams.csv\"  # Replace with your desired output CSV file path\n",
        "    n_grams = 5  # Set the desired value for n\n",
        "    print(current_index)\n",
        "    create_ngrams_index(input_csv, output_csv, n_grams)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqSMcwOMXAXF",
        "outputId": "9b5258f9-9e99-4d08-eb1f-8e63cf035c7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-25-8d94636e5db6>:73: RuntimeWarning: overflow encountered in multiply\n",
            "  self.W_grad[target_word_index] += diff * self.W[context_word_index]\n",
            "<ipython-input-25-8d94636e5db6>:74: RuntimeWarning: overflow encountered in multiply\n",
            "  self.W_grad[context_word_index] += diff * self.W[target_word_index]\n",
            "<ipython-input-25-8d94636e5db6>:79: RuntimeWarning: invalid value encountered in subtract\n",
            "  self.W -= self.learning_rate * self.W_grad\n",
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##GLOVE ALGO\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "updated_file_path = r'bbbb_ngrams.csv'\n",
        "df = pd.read_csv(updated_file_path, header=None, names=['text', 'label'], skiprows=1)\n",
        "# Assuming ngram_dict is the dictionary you want to check\n",
        "\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "# print(X)\n",
        "vocab_size = 6074\n",
        "\n",
        "\n",
        "\n",
        "sentences = X\n",
        "\n",
        "# Define window size and initialize co-occurrence matrix\n",
        "window_size = 5\n",
        "co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
        "\n",
        "# Iterate over sentences to fill the co-occurrence matrix\n",
        "\n",
        "# Iterate over sentences to fill the co-occurrence matrix\n",
        "# Iterate over sentences to fill the co-occurrence matrix\n",
        "\n",
        "\n",
        "# Iterate over sentences to fill the co-occurrence matrix\n",
        "for sentence in sentences:\n",
        "    for center_word, context_word in combinations(sentence, 2):\n",
        "        try:\n",
        "            center_word_index = int(center_word)  # Try to convert to integer\n",
        "            context_word_index = int(context_word)  # Try to convert to integer\n",
        "        except ValueError:\n",
        "            # If conversion to integer fails, continue to the next iteration\n",
        "            continue\n",
        "\n",
        "        # Now, you can use center_word_index and context_word_index in your logic\n",
        "        co_occurrence_matrix[center_word_index, context_word_index] += 1\n",
        "        co_occurrence_matrix[context_word_index, center_word_index] += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a simple GloVe model\n",
        "class GloveModel:\n",
        "    def __init__(self, vocab_size, vector_size=100, learning_rate=0.05):\n",
        "        self.W = np.random.rand(vocab_size, vector_size).astype(np.float32)\n",
        "        self.b = np.random.rand(vocab_size).astype(np.float32)\n",
        "        self.W_grad = np.zeros_like(self.W)\n",
        "        self.b_grad = np.zeros_like(self.b)\n",
        "        self.vector_size = vector_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def train_step(self, target_word_index, context_word_index, co_occurrence_count):\n",
        "        # Calculate predicted co-occurrence count\n",
        "        prediction = np.dot(self.W[target_word_index], self.W[context_word_index]) + self.b[target_word_index] + self.b[context_word_index]\n",
        "        diff = prediction - np.log(co_occurrence_count)\n",
        "\n",
        "        # Update gradients\n",
        "        self.W_grad[target_word_index] += diff * self.W[context_word_index]\n",
        "        self.W_grad[context_word_index] += diff * self.W[target_word_index]\n",
        "        self.b_grad[target_word_index] += diff\n",
        "        self.b_grad[context_word_index] += diff\n",
        "\n",
        "    def update_params(self):\n",
        "        self.W -= self.learning_rate * self.W_grad\n",
        "        self.b -= self.learning_rate * self.b_grad\n",
        "        self.W_grad.fill(0)\n",
        "        self.b_grad.fill(0)\n",
        "\n",
        "# Train the GloVe model using the co-occurrence matrix\n",
        "glove_model = GloveModel(vocab_size, vector_size=100, learning_rate=0.05)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(vocab_size):\n",
        "            if co_occurrence_matrix[i, j] > 0:\n",
        "                glove_model.train_step(i, j, co_occurrence_matrix[i, j])\n",
        "\n",
        "    glove_model.update_params()\n",
        "\n",
        "# Save the trained word embeddings\n",
        "word_embeddings = glove_model.W\n",
        "\n",
        "# Optionally, you can use the trained word embeddings for downstream tasks\n",
        "# For example, you can use gensim's Word2Vec wrapper to load the embeddings\n",
        "gensim_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "gensim_model.wv.vectors = word_embeddings\n",
        "gensim_model.save('glove_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVqvbThmfnjc",
        "outputId": "0733d4d6-a244-474e-b99d-23c0385fe42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 11s 167ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 4s 95ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 4s 107ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 3s 74ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 3s 75ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 3s 73ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 4s 111ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 3s 75ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 3s 75ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 3s 76ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 4s 108ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 3s 74ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 3s 76ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 3s 76ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 4s 107ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 3s 74ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 3s 74ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 3s 74ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 4s 109ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 3s 73ms/step - loss: nan - accuracy: 0.9861 - val_loss: nan - val_accuracy: 0.9837\n",
            "12/12 [==============================] - 0s 26ms/step - loss: nan - accuracy: 1.0000\n",
            "Test Loss: nan, Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Load the saved GloVe model\n",
        "glove_model = Word2Vec.load('glove_model')\n",
        "\n",
        "# Convert list of strings to a list of lists (as Tokenizer expects a list of strings)\n",
        "X = [[str(word) for word in str(sentence).split()] for sentence in X]\n",
        "\n",
        "# Tokenize the text and convert it into sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to have consistent length\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert string labels to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Convert numerical labels to one-hot encoding\n",
        "labels_one_hot = to_categorical(y_encoded)\n",
        "\n",
        "# Create LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length, weights=[glove_model.wv.vectors], trainable=False))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcLh1yUdfnmS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fntd2MCGXAbm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g_3kvW2XAex"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUVR7zmdGN87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdaxywOBHolo",
        "outputId": "3707aa6e-17ba-4dd3-de27-080fb76aa888"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "42/42 [==============================] - 10s 189ms/step - loss: 0.8018 - accuracy: 0.6401 - val_loss: 0.6917 - val_accuracy: 0.7049\n",
            "Epoch 2/10\n",
            "42/42 [==============================] - 6s 141ms/step - loss: 0.6280 - accuracy: 0.7325 - val_loss: 0.6671 - val_accuracy: 0.6997\n",
            "Epoch 3/10\n",
            "42/42 [==============================] - 7s 173ms/step - loss: 0.5675 - accuracy: 0.7593 - val_loss: 0.6208 - val_accuracy: 0.7153\n",
            "Epoch 4/10\n",
            "42/42 [==============================] - 6s 151ms/step - loss: 0.5193 - accuracy: 0.7765 - val_loss: 0.6320 - val_accuracy: 0.7222\n",
            "Epoch 5/10\n",
            "42/42 [==============================] - 7s 176ms/step - loss: 0.4910 - accuracy: 0.7906 - val_loss: 0.6053 - val_accuracy: 0.7344\n",
            "Epoch 6/10\n",
            "42/42 [==============================] - 8s 181ms/step - loss: 0.4567 - accuracy: 0.8137 - val_loss: 0.5964 - val_accuracy: 0.7431\n",
            "Epoch 7/10\n",
            "42/42 [==============================] - 6s 152ms/step - loss: 0.4257 - accuracy: 0.8256 - val_loss: 0.6945 - val_accuracy: 0.7188\n",
            "Epoch 8/10\n",
            "42/42 [==============================] - 8s 183ms/step - loss: 0.4094 - accuracy: 0.8264 - val_loss: 0.5567 - val_accuracy: 0.7674\n",
            "Epoch 9/10\n",
            "42/42 [==============================] - 6s 153ms/step - loss: 0.3552 - accuracy: 0.8651 - val_loss: 0.5933 - val_accuracy: 0.7483\n",
            "Epoch 10/10\n",
            "42/42 [==============================] - 8s 184ms/step - loss: 0.3419 - accuracy: 0.8584 - val_loss: 0.5961 - val_accuracy: 0.7517\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.5961 - accuracy: 0.7517\n",
            "Test Accuracy: 75.17%\n"
          ]
        }
      ],
      "source": [
        "##PRETRAINED MODEL\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "file_path = r'hindi.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Tokenize input text\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
        "\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "max_length = 128  # Adjust as needed\n",
        "tokenized_input = tokenizer(X, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "    word_embeddings = model(**tokenized_input).last_hidden_state\n",
        "\n",
        "# Convert PyTorch tensor to NumPy array\n",
        "word_embeddings_np = word_embeddings.numpy()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(word_embeddings_np, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(100))\n",
        "lstm_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train, y_train_onehot, epochs=10, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = lstm_model.evaluate(X_test, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IHBd0jbfNj28",
        "outputId": "930b104d-be15-4a8f-a2b2-9fdb80bdfbb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector size of indic-bert model: 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "42/42 [==============================] - 10s 158ms/step - loss: 0.8288 - accuracy: 0.6326 - val_loss: 0.6860 - val_accuracy: 0.7083\n",
            "Epoch 2/20\n",
            "42/42 [==============================] - 7s 170ms/step - loss: 0.6429 - accuracy: 0.7295 - val_loss: 0.6572 - val_accuracy: 0.7083\n",
            "Epoch 3/20\n",
            "42/42 [==============================] - 6s 143ms/step - loss: 0.5870 - accuracy: 0.7466 - val_loss: 0.6765 - val_accuracy: 0.6962\n",
            "Epoch 4/20\n",
            "42/42 [==============================] - 7s 170ms/step - loss: 0.5530 - accuracy: 0.7683 - val_loss: 0.6259 - val_accuracy: 0.7257\n",
            "Epoch 5/20\n",
            "42/42 [==============================] - 6s 151ms/step - loss: 0.4832 - accuracy: 0.8010 - val_loss: 0.5920 - val_accuracy: 0.7222\n",
            "Epoch 6/20\n",
            "42/42 [==============================] - 8s 181ms/step - loss: 0.4414 - accuracy: 0.8100 - val_loss: 0.5767 - val_accuracy: 0.7413\n",
            "Epoch 7/20\n",
            "42/42 [==============================] - 6s 152ms/step - loss: 0.4280 - accuracy: 0.8167 - val_loss: 0.5966 - val_accuracy: 0.7170\n",
            "Epoch 8/20\n",
            "42/42 [==============================] - 7s 179ms/step - loss: 0.4065 - accuracy: 0.8301 - val_loss: 0.6135 - val_accuracy: 0.7535\n",
            "Epoch 9/20\n",
            "42/42 [==============================] - 6s 151ms/step - loss: 0.3751 - accuracy: 0.8450 - val_loss: 0.5809 - val_accuracy: 0.7587\n",
            "Epoch 10/20\n",
            "42/42 [==============================] - 7s 167ms/step - loss: 0.3393 - accuracy: 0.8554 - val_loss: 0.5499 - val_accuracy: 0.7708\n",
            "Epoch 11/20\n",
            "42/42 [==============================] - 8s 189ms/step - loss: 0.3142 - accuracy: 0.8644 - val_loss: 0.5885 - val_accuracy: 0.7674\n",
            "Epoch 12/20\n",
            "42/42 [==============================] - 6s 151ms/step - loss: 0.3324 - accuracy: 0.8599 - val_loss: 0.5878 - val_accuracy: 0.7622\n",
            "Epoch 13/20\n",
            "42/42 [==============================] - 7s 180ms/step - loss: 0.2891 - accuracy: 0.8815 - val_loss: 0.6119 - val_accuracy: 0.7587\n",
            "Epoch 14/20\n",
            "42/42 [==============================] - 6s 142ms/step - loss: 0.2479 - accuracy: 0.8957 - val_loss: 0.5680 - val_accuracy: 0.7760\n",
            "Epoch 15/20\n",
            "42/42 [==============================] - 7s 180ms/step - loss: 0.2209 - accuracy: 0.9232 - val_loss: 0.6523 - val_accuracy: 0.7674\n",
            "Epoch 16/20\n",
            "42/42 [==============================] - 6s 152ms/step - loss: 0.2275 - accuracy: 0.9054 - val_loss: 0.6380 - val_accuracy: 0.7691\n",
            "Epoch 17/20\n",
            "42/42 [==============================] - 8s 182ms/step - loss: 0.2472 - accuracy: 0.9136 - val_loss: 0.5945 - val_accuracy: 0.7674\n",
            "Epoch 18/20\n",
            "42/42 [==============================] - 6s 155ms/step - loss: 0.2579 - accuracy: 0.8979 - val_loss: 0.5949 - val_accuracy: 0.7778\n",
            "Epoch 19/20\n",
            "42/42 [==============================] - 7s 169ms/step - loss: 0.1860 - accuracy: 0.9247 - val_loss: 0.7368 - val_accuracy: 0.7587\n",
            "Epoch 20/20\n",
            "42/42 [==============================] - 8s 203ms/step - loss: 0.2245 - accuracy: 0.9113 - val_loss: 0.6661 - val_accuracy: 0.7361\n",
            "18/18 [==============================] - 1s 52ms/step - loss: 0.6661 - accuracy: 0.7361\n",
            "Test Accuracy: 73.61%\n"
          ]
        }
      ],
      "source": [
        "##PRETRAINED MODEL\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "file_path = r'hindi.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Tokenize input text\n",
        "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
        "\n",
        "X = df['text'].tolist()\n",
        "y = df['label'].tolist()\n",
        "\n",
        "max_length = 128  # Adjust as needed\n",
        "tokenized_input = tokenizer(X, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "    word_embeddings = model(**tokenized_input).last_hidden_state\n",
        "\n",
        "# Convert PyTorch tensor to NumPy array\n",
        "word_embeddings_np = word_embeddings.numpy()\n",
        "vector_size = word_embeddings_np.shape[-1]\n",
        "print(f\"Vector size of indic-bert model: {vector_size}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(word_embeddings_np, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "y_test = np.array(y_test)\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Build an LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(100))\n",
        "lstm_model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes: positive, negative, neutral\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train, y_train_onehot, epochs=20, validation_data=(X_test, y_test_onehot))\n",
        "\n",
        "# Evaluate on test data\n",
        "accuracy = lstm_model.evaluate(X_test, y_test_onehot)[1]\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaJNJG90pB5f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}